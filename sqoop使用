sqoop使用
官网介绍：
Sqoop是一种旨在在Hadoop与关系数据库或大型机之间传输数据的工具。您可以使用Sqoop将数据从MySQL或Oracle等关系数据库管理系统（RDBMS）或大型机导入Hadoop分布式文件系统（HDFS），在Hadoop MapReduce中转换数据，然后将数据导出回RDBMS 。
Sqoop依靠数据库描述要导入的数据的模式来自动执行此过程的大部分时间。Sqoop使用MapReduce导入和导出数据，这提供了并行操作以及容错能力。

案例参考文档：
https://blog.csdn.net/qq_36881881/article/details/88887243


示例1:
将数据全量抽取到aws S3同时自定义输出文件名:
sqoop import -D mapreduce.output.basename=`date +%Y-%m-%d` \
--connect xxxx --username xxxx --password xxxx --table Test --target-dir s3a://xxxx桶名/xxxx --fields-terminated-by ',' -m 1

实例2:
全量抽取数据,使用本地模式,利用linux 定时任务跑编写shell:

#!/bin/bash
export HADOOP_USER_NAME=xxxx
dayno=$(date "+%Y%m%d")
sqoop import -jt local --connect jdbc:oracle:xxxxxxxxxxxxx --username xxxx --password xxxx  --table xxxx --columns "字段A,字段B,字段C,字段D" --target-dir /xxxx/dayno=${dayno}  --m 1 &


实例3:
增量抽取数据,通过sqoop管理增量指标
查看sqoop job:
sqoop job -list

##创建测试任务
sqoop job --create testjob1 -- import -D mapreduce.output.basename=`date +%Y-%m-%d` \
--connect 'jdbc信息' \
--username xxxx \
--password-file file:///home/stat/job.password \
--table xxxx \
--target-dir  /xxxx/xxxx(hdfs目录) \
--temporary-rootdir /xxxx/xxxx(hdfs目录) \
--check-column id \
--last-value 0 \
--incremental append \
--fields-terminated-by ',' \
-m 1

说明:
--password-file 这里是将密码放到文件(权限为400)中 
--target-dir  数据写入的目标目录
--temporary-rootdir  运行需要指定临时目录
--check-column  通过检查指定的字段来增量抽取

定时执行脚本:sqoop job -exec testjob1 
