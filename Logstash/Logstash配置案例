#Logstash 配置细节
官方文档：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html

创建一个名为“logstash-simple.conf”的文件

实例1.标准配置：
input { stdin { } }
output {
  stdout { codec => rubydebug }
}

实现在控制台输入，控制台输出结果

启动logstash命令：
/bin/logstash -f logstash-simple.conf

运行多实例时请保证每个实例的path.data配置不一样
/bin/logstash  -f test.conf  --path.data=/*/*

path.data这个参数可以是在config目录下logstash.yml配置文件

实例2：获取日志文件过滤，实现使用multiline处理多行日志，实现只输出message
multiline 多行插件还有这几个属性：
max_bytes:  默认值10M  对内容大小限制 
max_lines: 默认值500   对行数进行限定
multiline_tag: 事件标志表明该数据是多行任务的结果 默认是添加一个字段
pattern:  实现正则匹配下面格式的日志 ()：
lnx2219 | CHANGED => {
    "changed": true, 
    "rc": 0, 
    "stderr": "Shared connection to lnx2219 closed.\r\n", 
    ................................
    "------JAVA------",
        "java version \"1.7.0_09-icedtea\"", 
        "OpenJDK Runtime Environment (rhel-2.3.4.1.el6_3-x86_64)", 
        "OpenJDK 64-Bit Server VM (build 23.2-b09, mixed mode)", 
        "------DB------",
        "------Mid------",
        "------END------"
    ]
}


input{
        file {
        path => "/home/logs/*.log"
        start_position => beginning
        stat_interval => 5
        codec => multiline {
                pattern => "(^[0-9a-zA-Z\_]+\s\|\s[A-Z]+\s=>\s)({\s\S+$})"
                what => "previous"
                negate => true
		max_lines => 1000
        }
    }
}
output{
   file {
        path => "/home/data/%{+yyyy}/%{+MM}/%{+dd-HH}.log"
		    codec => line { format => " message: %{message}"}//只输出message
    }
}


实例3.日志文件中获取数据，输出到ES里
input{
        file {
        path => "/home/logs/*.log"
        start_position => beginning
        stat_interval => 5
        codec => multiline {
                pattern => "(^[0-9a-zA-Z\_]+\s\|\s[A-Z]+\s=>\s)({\s\S+$})"
                what => "previous"
                negate => true
        }
    }
}
output {
    elasticsearch {
        host => “X.X.X.X“         # 或者cluster => “ClusterName“ 
        protocol => "http"
        index => "test_output-%{type}-%{+YYYY.MM.dd}"
        document_type => "nginx"
        workers => 5  
	}
}


案例参考:
https://blog.csdn.net/alan_liuyue/article/details/91998144?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task

实例4:
将csv文件中的增量数据导入kafka
input {
  file {
    path => "/home/test/testdir/test-*.csv"
  }
}

output {
  kafka {
    bootstrap_servers => "hostname1:9092,hostname2:9092,hostname3:9092"
    topic_id => "hm_test"
  }
}


从kafka消费得到的消息是:
时间戳 hostname message

logstash-kafka 插件输入和输出默认 codec 为 json 格式.消息传递过程中 logstash 默认会为消息编码内加入相应的时间戳和 hostname 等信息.
如果只做消息转发,可以设置如下(放在kafka下级):
    codec => plain {
        format => "%{message}"
     }
message就是每一行的记录.

如果想要转成json输出设置:
codec => json 
则从kafka里消费得到:
{"@version":"1","path":"/home/xxx.log","message":"test11111111111111111111111","@timestamp":"2020-03-19T03:01:51.131Z","host":"xxx"}


将文件中的数据采集到ES中默认_source字段的结构是:
{
    "message" : "",
    "@timestamp" : "2020-04-11T15:05:35.310Z",
    "path" : "/xxx/yyy/test.log",
    "host" : "node-1",
    "@version" : "1"
}


使用ruby filter对数据进行重现构造.
在不同输入input中设置type,使用filter对event进行重新构造.
将csv数据构造成自定义event.修改字段名.
filter {
    ruby {
        code => '
	    event.set("server", event.get("host"))
            event.set("businessName",event.get("type"))
	'
    }
    mutate {
            remove_field => "type"
            remove_field => "host"
        }
}



收集csv 数据和apachelog 案例参考:
https://blog.csdn.net/wfs1994/article/details/80862541
